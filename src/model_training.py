import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import joblib

def train_models(train_df, optimize=False):
    """
    Entra√Ænement des mod√®les ML et sauvegarde des mod√®les entra√Æn√©s.
    
    Arguments:
    train_df -- DataFrame contenant les donn√©es pr√©trait√©es
    optimize -- Si True, recherche d'hyperparam√®tres avec GridSearchCV
    
    Retourne:
    logreg, rf, xgb_model -- Mod√®les entra√Æn√©s
    """
    X = train_df.drop(columns=['Survived'])
    y = train_df['Survived']

    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

    print("üì¢ Entra√Ænement du mod√®le de R√©gression Logistique...")
    logreg = LogisticRegression(max_iter=1000)
    if optimize:
        param_grid = {'C': [0.01, 0.1, 1, 10]}
        logreg = GridSearchCV(logreg, param_grid, cv=5)
    logreg.fit(X_train, y_train)

    print("üì¢ Entra√Ænement du mod√®le RandomForest...")
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)

    print("üì¢ Entra√Ænement du mod√®le XGBoost...")
    xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
    xgb_model.fit(X_train, y_train)

    print("‚úÖ Mod√®les entra√Æn√©s et sauvegard√©s !")
    joblib.dump(logreg, "models/logreg.pkl")
    joblib.dump(rf, "models/random_forest.pkl")
    joblib.dump(xgb_model, "models/xgboost.pkl")

    return logreg, rf, xgb_model
